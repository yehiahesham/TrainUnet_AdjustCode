{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "rewiting the old one to match the new interface while keeping compatibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make  `query` mehthod :\n",
    "1. as a static method and adapting it to use the new ModelHandler and DataSetHandler interfaces for operations related to model and dataset management.\n",
    "2. The new query method must return a tuple of lists, \n",
    "    * 1st list :  contains indices of selected samples from the unlabeled dataset,\n",
    "    * 2nd list : Additional info (in the context of BadgeSampling, this might not be necessary, so it could return an empty list).\n",
    "3. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gc\n",
    "import numpy as np\n",
    "import torch,time,tqdm,pickle,os,itertools\n",
    "import torch.nn.functional as F\n",
    "from sklearn.cluster import kmeans_plusplus\n",
    "from typing import List, Tuple, Dict, overload\n",
    "from torch import nn\n",
    "from abc import ABC, abstractmethod\n",
    "from torch.utils.data import DataLoader,Dataset\n",
    "from collections import Counter\n",
    "# from .strategy import Strategy\n",
    "# from pcdet.models import load_data_to_gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelHandler(): \n",
    "    def get_grad_embedding(self, model, probs, feat): \n",
    "        '''\n",
    "        creates gradient embedding from the  probabilities and  features.\n",
    "        creates embeddings that reflect the uncertainty . or simply info values based on their prob and feat\n",
    "\n",
    "        '''\n",
    "        embeddings_pos = feat * (1 - probs) # create an embedding emphasizing features model is uncertain off.\n",
    "\n",
    "        embeddings = torch.cat((embeddings_pos, -embeddings_pos), axis=-1) # represent both aspects of the prediction (e.g., for Binary Classification : one class vs. the other) in the embedding space.\n",
    "        final_embeddings = torch.clone(embeddings)\n",
    "         \n",
    "        # rearranges the embedding such that the negated part of the features comes first\n",
    "        # sawpping the 2 if the prob < 0.5\n",
    "        final_embeddings[probs < 0.5] = torch.cat((-embeddings_pos[probs < 0.5], embeddings_pos[probs < 0.5]), axis=1)\n",
    "        # B x 1 true false true\n",
    "\n",
    "        return embeddings #not final_embedding?\n",
    "    \n",
    "    def enable_dropout(self, model):\n",
    "        i = 0\n",
    "        for m in model.modules():\n",
    "            if m.__class__.__name__.startswith('Dropout'):\n",
    "                i += 1\n",
    "                m.train()\n",
    "        print('**found and enabled {} Dropout layers for random sampling**'.format(i))\n",
    "\n",
    "\n",
    "class DataSetHandler(): \n",
    "    unlabeled_idcs : List[int] \n",
    "    labeled_idcs : List[int] \n",
    "    \n",
    "    def __init__(self, unlabeled_dataset: Dataset, labeled_dataset: Dataset, batch_size: int, num_workers: int):\n",
    "        self.unlabeled_dataset = unlabeled_dataset\n",
    "        self.labeled_dataset = labeled_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.unlabeled_idcs = list(range(len(self.unlabeled_dataset)))\n",
    "        self.labeled_idcs = list(range(len(self.labeled_dataset)))\n",
    "\n",
    "\n",
    "    def get_unlabeled_loader(self): \n",
    "        return DataLoader(self.unlabeled_dataset, batch_size=self.batch_size, \n",
    "                          shuffle=False, num_workers=self.num_workers)\n",
    " \n",
    "     \n",
    "    def get_labeled_loader(self): \n",
    "        return DataLoader(self.labeled_dataset, batch_size=self.batch_size, \n",
    "                          shuffle=False, num_workers=self.num_workers)\n",
    " \n",
    "\n",
    "\n",
    "class StrategyNewInterface(ABC): \n",
    "    def __init__(self, *args, **kwarg): \n",
    "        pass \n",
    " \n",
    "    @overload \n",
    "    @staticmethod \n",
    "    @abstractmethod \n",
    "    def query(model : nn.Module, handler: ModelHandler, dataset_handler: DataSetHandler, \n",
    "             training_config: Dict, query_size: int, device, \n",
    "             *arg, **kwargs) -> Tuple[List, List]: \n",
    "\n",
    "       pass\n",
    "\n",
    "\n",
    "\n",
    "class BadgeSampling(StrategyNewInterface):\n",
    "\n",
    "    @staticmethod\n",
    "    def query(model: nn.Module, handler: ModelHandler, dataset_handler: DataSetHandler,\n",
    "              training_config: Dict, query_size: int, device,\n",
    "              *args, **kwargs) -> Tuple[List, List]:\n",
    "        \n",
    "        \n",
    "        rank        = training_config.get('rank', 0) \n",
    "        cur_epoch   = training_config.get.get('cur_epoch', 0)\n",
    "        leave_pbar  = training_config.get.get('leave_pbar', True)\n",
    "        save_points = training_config.get.get('save_points', lambda frame_id, pred_dict: None)\n",
    "        active_label_dir = training_config.get.get('active_label_dir', './')\n",
    "        \n",
    "        unlabelled_loader = dataset_handler.get_unlabeled_loader()\n",
    "        unlabelled_set  = dataset_handler.unlabeled_dataset\n",
    "\n",
    "        val_loader = unlabelled_loader\n",
    "        val_dataloader_iter = iter(val_loader)\n",
    "        total_it_each_epoch = len(val_loader)\n",
    "        \n",
    "        if rank == 0:\n",
    "            pbar = tqdm.tqdm(total=total_it_each_epoch, leave=leave_pbar,\n",
    "                             desc='evaluating_unlabelled_set_epoch_%d' % cur_epoch, dynamic_ncols=True)\n",
    "            \n",
    "        handler.enable_dropout(model) # Enable dropout for uncertainty estimation\n",
    "        rpn_preds_results = []\n",
    "\n",
    "        if os.path.isfile(os.path.join(active_label_dir, f'grad_embeddings_epoch_{cur_epoch}.pkl')):\n",
    "            print(f'found {cur_epoch} epoch grad embeddings... start resuming...')\n",
    "            with open(os.path.join(active_label_dir, f'grad_embeddings_epoch_{cur_epoch}.pkl'), 'rb') as f:\n",
    "                grad_embeddings = pickle.load(f)\n",
    "        else:\n",
    "            for cur_it in range(total_it_each_epoch):\n",
    "                try:\n",
    "                    unlabelled_batch = next(val_dataloader_iter)\n",
    "                except StopIteration:\n",
    "                    unlabelled_dataloader_iter = iter(val_loader)\n",
    "                    unlabelled_batch = next(unlabelled_dataloader_iter)\n",
    "                with torch.no_grad():\n",
    "                    # load_data_to_gpu(unlabelled_batch)\n",
    "                    pred_dicts, _ = model(unlabelled_batch)\n",
    "                    for batch_inx in range(len(pred_dicts)):\n",
    "                        save_points(unlabelled_batch['frame_id'][batch_inx], pred_dicts[batch_inx])\n",
    "                        # final_full_cls_logits = pred_dicts[batch_inx]['pred_logits']\n",
    "                    # did not apply batch mask -> directly output \n",
    "                    rpn_preds = pred_dicts[0]['rpn_preds']\n",
    "                    batch_size = rpn_preds.shape[0]\n",
    "                    rpn_preds = torch.argmax(rpn_preds.view(batch_size, -1, model.dense_head.num_class), -1)\n",
    "                    rpn_preds_results.append(rpn_preds.cpu())\n",
    "                if rank == 0:\n",
    "                    pbar.update()\n",
    "                    pbar.refresh()\n",
    "                        \n",
    "            if rank == 0:\n",
    "                pbar.close()\n",
    "            del rpn_preds\n",
    "            del pred_dicts\n",
    "            torch.cuda.empty_cache()\n",
    "            print('start stacking cls and reg results as gt...')\n",
    "            rpn_preds_results = torch.cat(rpn_preds_results, 0)\n",
    "\n",
    "            print('retrieving grads on the training mode...')\n",
    "            model.train()\n",
    "            rpn_grad_embedding_list = []\n",
    "\n",
    "            grad_loader = DataLoader(\n",
    "                unlabelled_set, batch_size=1, pin_memory=True, num_workers=dataset_handler.get_unlabeled_loader().num_workers,\n",
    "                shuffle=False, collate_fn=unlabelled_set.collate_batch,\n",
    "                drop_last=False, sampler=unlabelled_loader.sampler, timeout=0\n",
    "                )\n",
    "            grad_dataloader_iter = iter(grad_loader)\n",
    "            total_it_each_epoch = len(grad_loader)\n",
    "\n",
    "            if rank == 0:\n",
    "                pbar = tqdm.tqdm(total=total_it_each_epoch, leave=leave_pbar,\n",
    "                                desc='inf_grads_unlabelled_set_epoch_%d' % cur_epoch, dynamic_ncols=True)\n",
    "            \n",
    "            for cur_it in range(total_it_each_epoch):\n",
    "                try:\n",
    "                    unlabelled_batch = next(grad_dataloader_iter)\n",
    "                    \n",
    "                except StopIteration:\n",
    "                    unlabelled_dataloader_iter = iter(grad_loader)\n",
    "                    unlabelled_batch = next(grad_dataloader_iter)\n",
    "\n",
    "                # load_data_to_gpu(unlabelled_batch)\n",
    "                    \n",
    "                pred_dicts, _, _= model(unlabelled_batch)\n",
    "                \n",
    "                new_data  = {'box_cls_labels': rpn_preds_results[cur_it, :].cuda().unsqueeze(0), 'cls_preds': pred_dicts['rpn_preds']}\n",
    "                rpn_loss = model.dense_head.get_cls_layer_loss(new_data=new_data)[0]\n",
    "                # since the rpn head does not have dropout, we cannot get MC dropout labels for regression\n",
    "                loss = rpn_loss\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "\n",
    "                rpn_grads = model.dense_head.conv_cls.weight.grad.clone().detach().cpu()\n",
    "                rpn_grad_embedding_list.append(rpn_grads)\n",
    "                \n",
    "                if rank == 0:\n",
    "                    pbar.update()\n",
    "                    pbar.refresh()\n",
    "                    \n",
    "\n",
    "            if rank == 0:\n",
    "                pbar.close()\n",
    "\n",
    "        rpn_grad_embeddings = torch.stack(rpn_grad_embedding_list, 0) #rpn_grad_embedding_list\n",
    "        del rpn_grad_embedding_list\n",
    "        gc.collect()\n",
    "        num_sample = rpn_grad_embeddings.shape[0]\n",
    "        rpn_grad_embeddings = rpn_grad_embeddings.view(num_sample, -1)\n",
    "\n",
    "        start_time = time.time()\n",
    "        _, selected_rpn_idx = kmeans_plusplus(rpn_grad_embeddings.numpy(), n_clusters=query_size, random_state=0)\n",
    "        print(\"--- kmeans++ running time: %s seconds for rpn grads---\" % (time.time() - start_time))\n",
    "        \n",
    "        selected_idx = selected_rpn_idx\n",
    "        model.zero_grad()\n",
    "        model.eval()\n",
    "        selected_frames = [unlabelled_set.sample_id_list[idx] for idx in selected_idx]\n",
    "        return selected_frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adapter Pattern for Compatibility\n",
    "\n",
    "To ensure compatibility with the old interface, I created a new adapter class. This class will adapt the new interface to be compatible with the existing code that expects the old interface. \n",
    "\n",
    "Calling query abstracts the user whether it is called using the new or old interface. As it will try use old interface paramters to call the new interface query function. \n",
    "\n",
    "Thus backward compatible!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StrategyAdapter:\n",
    "    def __init__(self, model : nn.Module, new_strategy: StrategyNewInterface,\n",
    "                 unlabeled_dataset: Dataset, labeled_dataset: Dataset, \n",
    "                 batch_size: int, num_workers: int,cfg):\n",
    "        self.model=model\n",
    "        self.new_strategy = new_strategy\n",
    "        self.handler = ModelHandler()\n",
    "        self.dataset_handler = DataSetHandler(unlabeled_dataset, labeled_dataset, batch_size, num_workers)\n",
    "        self.cfg=cfg\n",
    " \n",
    "    def query(self,leave_pbar=True, cur_epoch=None):\n",
    "\n",
    "        # Assume training_config is part of cfg\n",
    "        training_config = self.cfg.get('training_config', {})\n",
    "        query_size = training_config.get('query_size', 10)  # Default query size\n",
    "        device=\"cuda\" if torch.cuda.is_available() else \"cpu\"        \n",
    "        training_config[\"leave_pbar\"]=leave_pbar\n",
    "        training_config[\"cur_epoch\"]=cur_epoch\n",
    "\n",
    "        # We assuming the following paramters are in training_config:\n",
    "        # training_config = {\n",
    "        #     \"rank\":0,\n",
    "        #     \"active_label_dir\": \"...\",\n",
    "        #     \"cur_epoch\": 0,\n",
    "        #     \"leave_pbar\": True,\n",
    "        #     \"save_points\": lambda frame_id, pred_dict: None,\n",
    "        #     # Other configuration settings...\n",
    "        # }\n",
    "        return self.new_strategy.query(\n",
    "            self.model, self.handler, self.dataset_handler,\n",
    "            training_config,query_size,device,)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MLHack",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
